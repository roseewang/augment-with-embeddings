{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import io\n",
    "import tiktoken\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from typing import Sequence\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(filename):\n",
    "    f = io.open(filename, mode=\"r\", encoding=\"utf-8\")\n",
    "    html_doc = f.read()\n",
    "    f.close()\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(text, max_token_count):\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    words = text.split()\n",
    "    chunk = []\n",
    "    encodeStr = \"\"\n",
    "    token_count = 0\n",
    "    \n",
    "    \n",
    "    for word in words:\n",
    "        if token_count+len(enc.encode(word))>max_token_count:\n",
    "            chunk.append(encodeStr.strip())\n",
    "            encodeStr = \"\"\n",
    "            token_count = 0\n",
    "        else:\n",
    "            encodeStr = encodeStr + \" \" + word\n",
    "            token_count += len(enc.encode(word))\n",
    "    \n",
    "    if encodeStr:        \n",
    "        chunk.append(encodeStr.strip())\n",
    "        \n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_chunk_text(directory, max_token_count):\n",
    "    results = {}\n",
    "    \n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.html'):\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                text = extract_text(file_path)\n",
    "                chunks = chunking(text, max_token_count)\n",
    "                results[file_path] = chunks\n",
    "                \n",
    "                \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embeddings_of_chunk_batch(batch_chunk,EMBEDDING_MODEL):\n",
    "    response = client.embeddings.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=batch_chunk\n",
    "    )\n",
    "\n",
    "    for i, embedding in enumerate(response.data):\n",
    "        assert i == embedding.index\n",
    "\n",
    "    return [e.embedding for e in response.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embeddings_for_dict(chunks_dict):\n",
    "    \"\"\"\n",
    "    Calculate embeddings for a dictionary where each key is a file path and the corresponding value is a list of text chunks.\n",
    "    \n",
    "    Parameters:\n",
    "    - chunks_dict (dict): Dictionary with file paths as keys and lists of text chunks as values.\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with file paths as keys and lists of embeddings as values.\n",
    "    \"\"\"\n",
    "\n",
    "    EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "    BATCH_SIZE = 20000\n",
    "    results = {}\n",
    "    \n",
    "    for path, chunks in chunks_dict.items():\n",
    "        embeddings = []\n",
    "        for x in range(BATCH_SIZE):\n",
    "            start = x * BATCH_SIZE\n",
    "            end = start + BATCH_SIZE\n",
    "            batch = chunks[start:end]\n",
    "            # print(\"Calculating embedding for batch #\", x)\n",
    "            embeddings.extend(calculate_embeddings_of_chunk_batch(batch,EMBEDDING_MODEL))\n",
    "            \n",
    "        results[path] = embeddings\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_for_chunks(chunks):\n",
    "    \"\"\"\n",
    "    Calculate embeddings for a list of text chunks.\n",
    "    \n",
    "    Parameters:\n",
    "    - chunks (list): List of text chunks.\n",
    "    \n",
    "    Returns:\n",
    "    - List of embeddings for each chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "    BATCH_SIZE = 20000\n",
    "    results = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        embeddings = []\n",
    "        for x in range(0,len(chunks),BATCH_SIZE):\n",
    "            start = x\n",
    "            end = start + BATCH_SIZE\n",
    "            batch = chunk[start:end]\n",
    "            # print(\"Calculating embedding for batch #\", x)\n",
    "            embeddings.extend(calculate_embeddings_of_chunk_batch(batch,EMBEDDING_MODEL))\n",
    "        results.append(embeddings)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory_and_get_embeddings(directory, max_tokens=4096):\n",
    "    all_chunks = []\n",
    "    file_paths = []\n",
    "    \n",
    "    file_to_chunks = extract_and_chunk_text(directory, max_tokens)\n",
    "    file_paths = list(file_to_chunks.keys())\n",
    "    all_chunks = list(file_to_chunks.values())\n",
    "    \n",
    "    embeddings = get_embeddings_for_chunks(all_chunks)\n",
    "    \n",
    "    return pd.DataFrame({'file_path':file_paths, 'text':all_chunks, 'embeddings': embeddings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"): \n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_strings(df, user_query, limit=100):\n",
    "    embedding = get_embedding(\n",
    "        user_query,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "\n",
    "    df[\"similarities\"] = df.embeddings.apply(lambda x: cosine_similarity(x, embedding))\n",
    "    res = df.sort_values(\"similarities\", ascending=False).head(limit)\n",
    "    return res[\"text\"], res[\"similarities\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder constants for models\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "# Partial function to count number of tokens\n",
    "def num_tokens(text, model=GPT_MODEL):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = len(encoding.encode(text))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial function to construct a query message\n",
    "def query_message(query, df, model, token_budget):\n",
    "    strings, relatednesses = search_similar_strings(df, query, token_budget)\n",
    "\n",
    "    intro = \"Using the provided documentation on LangChain, answer the following question. If the answer cannot be found in the documents, write 'I could not find an answer'.\\n\"\n",
    "    question = f\"Question: {query}\\n\"\n",
    "    documentation = f\"Documentation: {strings}\"\n",
    "    message = intro + question + documentation\n",
    "\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_model(query: str, model=GPT_MODEL, token_budget=4096-500, print_message=False) -> str:\n",
    "    df = process_directory_and_get_embeddings(\"apidocs\")\n",
    "    message = query_message(query, df, model, token_budget)\n",
    "\n",
    "    if print_message:\n",
    "        print(message)\n",
    "\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful teaching assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ]\n",
    "\n",
    "    max_tokens = token_budget - num_tokens(message) - num_tokens(\"You are a helpful teaching assistant.\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    response_message = response.choices[0].message.content\n",
    "\n",
    "    return response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
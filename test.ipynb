{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import io\n",
    "import tiktoken\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from openai import OpenAI\n",
    "from typing import Sequence\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(filename: str) -> str:\n",
    "    f = io.open(filename, mode=\"r\", encoding=\"utf-8\")\n",
    "    html_doc = f.read()\n",
    "    f.close()\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(text: str, max_token_count: int) -> list[str]:\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    words = text.split()\n",
    "    chunk = []\n",
    "    encodeStr = \"\"\n",
    "    token_count = 0\n",
    "    \n",
    "    \n",
    "    for word in words:\n",
    "        if token_count+len(enc.encode(word))>max_token_count:\n",
    "            chunk.append(encodeStr.strip())\n",
    "            encodeStr = \"\"\n",
    "            token_count = 0\n",
    "        else:\n",
    "            encodeStr = encodeStr + \" \" + word\n",
    "            token_count += len(enc.encode(word))\n",
    "    \n",
    "    if encodeStr:        \n",
    "        chunk.append(encodeStr.strip())\n",
    "        \n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_chunk_text(directory: str, max_token_count: int) -> dict[str, list[str]]:\n",
    "    results = {}\n",
    "    \n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.html'):\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                text = extract_text(file_path)\n",
    "                chunks = chunking(text, max_token_count)\n",
    "                results[file_path] = chunks\n",
    "                \n",
    "                \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "type Vector = list[float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embeddings_of_chunk_batch(batch_chunk: list[str], EMBEDDING_MODEL: str) -> list[Vector]:\n",
    "    response = client.embeddings.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=batch_chunk\n",
    "    )\n",
    "\n",
    "    for i, embedding in enumerate(response.data):\n",
    "        assert i == embedding.index\n",
    "\n",
    "    return [e.embedding for e in response.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embeddings_for_dict(chunks_dict: dict[str, list[str]]) -> dict[str, list[Vector]]:\n",
    "    \"\"\"\n",
    "    Calculate embeddings for a dictionary where each key is a file path and the corresponding value is a list of text chunks.\n",
    "    \n",
    "    Parameters:\n",
    "    - chunks_dict (dict): Dictionary with file paths as keys and lists of text chunks as values.\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with file paths as keys and lists of embeddings as values.\n",
    "    \"\"\"\n",
    "\n",
    "    EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "    BATCH_SIZE = 1000\n",
    "    results = {}\n",
    "    \n",
    "    for path, chunks in chunks_dict.items():\n",
    "        embeddings = []\n",
    "        for x in range(BATCH_SIZE):\n",
    "            start = x * BATCH_SIZE\n",
    "            end = start + BATCH_SIZE\n",
    "            batch = chunks[start:end]\n",
    "            # print(\"Calculating embedding for batch #\", x)\n",
    "            embeddings.extend(calculate_embeddings_of_chunk_batch(batch,EMBEDDING_MODEL))\n",
    "            \n",
    "        results[path] = embeddings\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_for_chunks(chunks: list[str]) -> list[Vector]:\n",
    "    \"\"\"\n",
    "    Calculate embeddings for a list of text chunks.\n",
    "    \n",
    "    Parameters:\n",
    "    - chunks (list): List of text chunks.\n",
    "    \n",
    "    Returns:\n",
    "    - List of embeddings for each chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "    BATCH_SIZE = 1000\n",
    "    results = []\n",
    "    \n",
    "    for x in range(0,len(chunks),BATCH_SIZE):\n",
    "        start = x\n",
    "        end = start + BATCH_SIZE\n",
    "        batch = chunks[start:end]\n",
    "        # print(\"Calculating embedding for batch #\", x)\n",
    "        results.extend(calculate_embeddings_of_chunk_batch(batch, EMBEDDING_MODEL))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory_and_get_embeddings(directory: str, max_tokens: int =4096) -> pd.DataFrame:\n",
    "    all_chunks = []\n",
    "    file_paths = []\n",
    "    \n",
    "    file_to_chunks = extract_and_chunk_text(directory, max_tokens)\n",
    "    for file_path, chunks in file_to_chunks.items():\n",
    "        all_chunks.extend(chunks)\n",
    "        file_paths.extend([file_path] * len(chunks))\n",
    "    \n",
    "    embeddings = get_embeddings_for_chunks(all_chunks)\n",
    "    \n",
    "    return pd.DataFrame({'file_path':file_paths, 'text':all_chunks, 'embedding': embeddings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a: Vector, b: Vector) -> float:\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"): \n",
    "    return client.embeddings.create(input = text, model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_strings(df: pd.DataFrame, user_query: str, limit: int =100) -> tuple[list[str], list[float]]:\n",
    "    embedding = get_embedding(\n",
    "        user_query,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "\n",
    "    df[\"similarities\"] = df.embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
    "    res = df.sort_values(\"similarities\", ascending=False).head(limit)\n",
    "    return res[\"text\"], res[\"similarities\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder constants for models\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "# Partial function to count number of tokens\n",
    "def num_tokens(text, model=GPT_MODEL):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = len(encoding.encode(text))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial function to construct a query message\n",
    "def query_message(query: str, df: pd.DataFrame, token_budget: int) -> str:\n",
    "    strings, relatednesses = search_similar_strings(df, query, token_budget)\n",
    "\n",
    "    intro = \"Using the provided documentation on LangChain, answer the following question. If the answer cannot be found in the documents, write 'I could not find an answer'.\\n\"\n",
    "    question = f\"Question: {query}\\n\"\n",
    "    documentation = f\"Documentation: {strings}\"\n",
    "    message = intro + question + documentation\n",
    "\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_model(query: str, df: pd.DataFrame, model: str =GPT_MODEL, token_budget: int =4096-500, print_message: bool =False) -> str:\n",
    "    message = query_message(query, df, token_budget)\n",
    "\n",
    "    if print_message:\n",
    "        print(message)\n",
    "\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You answer questions about LangChain.\"},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ]\n",
    "\n",
    "    max_tokens = token_budget - num_tokens(message) - num_tokens(\"You answer questions about LangChain.\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    response_message = response.choices[0].message.content\n",
    "\n",
    "    return response_message"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.12.4 64-bit ('env')",
   "metadata": {
    "interpreter": {
     "hash": "ccd52d6fd44e5d2e3db85f734fb693c39446440a3ab7351d33381d606b77c829"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}